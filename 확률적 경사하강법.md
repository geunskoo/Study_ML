# 목표
머신러닝에 추가적인 자료가 계속 공급이 된다면, 계속 새로운 모델을 만들어 학습을 시켜야할까 ?

이를 해결하기 위한

확률적 경사 하강법을 이용한 머신러닝 학습 알고리즘에 대해서 알아보자!

## ML에 필요한 패키지 삽입


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
```

## 데이터 가공


```python
fish  = pd.read_csv("https://bit.ly/fish_csv_data")
fish.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
  </tbody>
</table>
</div>




```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```

훈련 세트와 테스트 세트로 나누자!!


```python
train_input,test_input,train_target,test_target = train_test_split(fish_input,fish_target,random_state = 42)
```

훈련 세트와 테스트 세트 표준화 전처리


```python
ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

## 확률적 경사하강법을 제공하는 분류 클래스 SGDClassifier를 사용해보자!


```python
from sklearn.linear_model import SGDClassifier
```


```python
sc = SGDClassifier(loss = 'log',max_iter = 10, random_state = 42)
sc.fit(train_scaled,train_target)

print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

    0.773109243697479
    0.775


    C:\Anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(


훈련세트와 테스트 세트의 정확도가 낮다.

지정한 반복 횟수 10번이 부족해 보인다.

SGDClassifier 객체를 다시 만들지 않고 sc모델에 추가적으로 더 훈련을 해보자!


```python
sc.partial_fit(train_scaled,train_target)

print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

    0.8151260504201681
    0.85


에포크를 한번더 실행하니 정확도가 향상되었다!

* 에포크 횟수가 작다 - 모델이 훈련세트를 덜 학습한다. 산(손실함수)을 다 내려오지 못하고 훈련을 마치는 셈.과소적합.
* 에포크 횟수가 많다 - 모델이 훈련세트에 너무 잘 맞아 테스트 세트 점수가 떨어질수도 있음. 과대적합.

에포크 횟수가 증가함에 따라서 과소 적합을 문제가 해결이 된다. 그리고 테스트세트 점수가 어느순간 부터 감소하는데(과대적합)
감소하기 직전의 순간 훈련을 멈추는 것을 `조기 종료` 라고 한다.


```python
sc = SGDClassifier(loss = 'log',random_state = 42)

train_score = []
test_score = []
classes = np.unique(train_target)

for _ in range(300):
    sc.partial_fit(train_scaled,train_target,classes = classes)
    train_score.append(sc.score(train_scaled,train_target))
    test_score.append(sc.score(test_scaled,test_target))
    
    
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```


​    
![output_19_0](https://user-images.githubusercontent.com/97498405/160234559-a0aff0dc-239d-470e-ba7e-6c12002a3f1d.png)
​    


100번의 반복에서 가장 과대적합/과소적합의 균형이 맞춰짐을 확인 할 수 있다!


```python
sc = SGDClassifier(loss = 'log',max_iter = 100,tol=None,random_state = 42)
sc.fit(train_scaled,train_target)

print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))
```

    0.957983193277311
    0.925

