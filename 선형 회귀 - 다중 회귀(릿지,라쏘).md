# 목표

## ML에 필요한 패키지 삽입


```python
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

import matplotlib.pyplot as plt
```

## 다중 회귀를 위한 클래스 삽입


```python
from sklearn.preprocessing import PolynomialFeatures

#특성들을 제곱해주기도 하고 각각의 곱을 만들기도 하면서 다양한 특성들을 생성시켜줌.
```

## 데이터 불러오기


```python
df = pd.read_csv('https://bit.ly/perch_csv_data')
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>length</th>
      <th>height</th>
      <th>width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8.4</td>
      <td>2.11</td>
      <td>1.41</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13.7</td>
      <td>3.53</td>
      <td>2.00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15.0</td>
      <td>3.82</td>
      <td>2.43</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.2</td>
      <td>4.59</td>
      <td>2.63</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.4</td>
      <td>4.59</td>
      <td>2.94</td>
    </tr>
    <tr>
      <th>5</th>
      <td>18.0</td>
      <td>5.22</td>
      <td>3.32</td>
    </tr>
    <tr>
      <th>6</th>
      <td>18.7</td>
      <td>5.20</td>
      <td>3.12</td>
    </tr>
    <tr>
      <th>7</th>
      <td>19.0</td>
      <td>5.64</td>
      <td>3.05</td>
    </tr>
    <tr>
      <th>8</th>
      <td>19.6</td>
      <td>5.14</td>
      <td>3.04</td>
    </tr>
    <tr>
      <th>9</th>
      <td>20.0</td>
      <td>5.08</td>
      <td>2.77</td>
    </tr>
    <tr>
      <th>10</th>
      <td>21.0</td>
      <td>5.69</td>
      <td>3.56</td>
    </tr>
    <tr>
      <th>11</th>
      <td>21.0</td>
      <td>5.92</td>
      <td>3.31</td>
    </tr>
    <tr>
      <th>12</th>
      <td>21.0</td>
      <td>5.69</td>
      <td>3.67</td>
    </tr>
    <tr>
      <th>13</th>
      <td>21.3</td>
      <td>6.38</td>
      <td>3.53</td>
    </tr>
    <tr>
      <th>14</th>
      <td>22.0</td>
      <td>6.11</td>
      <td>3.41</td>
    </tr>
    <tr>
      <th>15</th>
      <td>22.0</td>
      <td>5.64</td>
      <td>3.52</td>
    </tr>
    <tr>
      <th>16</th>
      <td>22.0</td>
      <td>6.11</td>
      <td>3.52</td>
    </tr>
    <tr>
      <th>17</th>
      <td>22.0</td>
      <td>5.88</td>
      <td>3.52</td>
    </tr>
    <tr>
      <th>18</th>
      <td>22.0</td>
      <td>5.52</td>
      <td>4.00</td>
    </tr>
    <tr>
      <th>19</th>
      <td>22.5</td>
      <td>5.86</td>
      <td>3.62</td>
    </tr>
    <tr>
      <th>20</th>
      <td>22.5</td>
      <td>6.79</td>
      <td>3.62</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22.7</td>
      <td>5.95</td>
      <td>3.63</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23.0</td>
      <td>5.22</td>
      <td>3.63</td>
    </tr>
    <tr>
      <th>23</th>
      <td>23.5</td>
      <td>6.28</td>
      <td>3.72</td>
    </tr>
    <tr>
      <th>24</th>
      <td>24.0</td>
      <td>7.29</td>
      <td>3.72</td>
    </tr>
    <tr>
      <th>25</th>
      <td>24.0</td>
      <td>6.38</td>
      <td>3.82</td>
    </tr>
    <tr>
      <th>26</th>
      <td>24.6</td>
      <td>6.73</td>
      <td>4.17</td>
    </tr>
    <tr>
      <th>27</th>
      <td>25.0</td>
      <td>6.44</td>
      <td>3.68</td>
    </tr>
    <tr>
      <th>28</th>
      <td>25.6</td>
      <td>6.56</td>
      <td>4.24</td>
    </tr>
    <tr>
      <th>29</th>
      <td>26.5</td>
      <td>7.17</td>
      <td>4.14</td>
    </tr>
    <tr>
      <th>30</th>
      <td>27.3</td>
      <td>8.32</td>
      <td>5.14</td>
    </tr>
    <tr>
      <th>31</th>
      <td>27.5</td>
      <td>7.17</td>
      <td>4.34</td>
    </tr>
    <tr>
      <th>32</th>
      <td>27.5</td>
      <td>7.05</td>
      <td>4.34</td>
    </tr>
    <tr>
      <th>33</th>
      <td>27.5</td>
      <td>7.28</td>
      <td>4.57</td>
    </tr>
    <tr>
      <th>34</th>
      <td>28.0</td>
      <td>7.82</td>
      <td>4.20</td>
    </tr>
    <tr>
      <th>35</th>
      <td>28.7</td>
      <td>7.59</td>
      <td>4.64</td>
    </tr>
    <tr>
      <th>36</th>
      <td>30.0</td>
      <td>7.62</td>
      <td>4.77</td>
    </tr>
    <tr>
      <th>37</th>
      <td>32.8</td>
      <td>10.03</td>
      <td>6.02</td>
    </tr>
    <tr>
      <th>38</th>
      <td>34.5</td>
      <td>10.26</td>
      <td>6.39</td>
    </tr>
    <tr>
      <th>39</th>
      <td>35.0</td>
      <td>11.49</td>
      <td>7.80</td>
    </tr>
    <tr>
      <th>40</th>
      <td>36.5</td>
      <td>10.88</td>
      <td>6.86</td>
    </tr>
    <tr>
      <th>41</th>
      <td>36.0</td>
      <td>10.61</td>
      <td>6.74</td>
    </tr>
    <tr>
      <th>42</th>
      <td>37.0</td>
      <td>10.84</td>
      <td>6.26</td>
    </tr>
    <tr>
      <th>43</th>
      <td>37.0</td>
      <td>10.57</td>
      <td>6.37</td>
    </tr>
    <tr>
      <th>44</th>
      <td>39.0</td>
      <td>11.14</td>
      <td>7.49</td>
    </tr>
    <tr>
      <th>45</th>
      <td>39.0</td>
      <td>11.14</td>
      <td>6.00</td>
    </tr>
    <tr>
      <th>46</th>
      <td>39.0</td>
      <td>12.43</td>
      <td>7.35</td>
    </tr>
    <tr>
      <th>47</th>
      <td>40.0</td>
      <td>11.93</td>
      <td>7.11</td>
    </tr>
    <tr>
      <th>48</th>
      <td>40.0</td>
      <td>11.73</td>
      <td>7.22</td>
    </tr>
    <tr>
      <th>49</th>
      <td>40.0</td>
      <td>12.38</td>
      <td>7.46</td>
    </tr>
    <tr>
      <th>50</th>
      <td>40.0</td>
      <td>11.14</td>
      <td>6.63</td>
    </tr>
    <tr>
      <th>51</th>
      <td>42.0</td>
      <td>12.80</td>
      <td>6.87</td>
    </tr>
    <tr>
      <th>52</th>
      <td>43.0</td>
      <td>11.93</td>
      <td>7.28</td>
    </tr>
    <tr>
      <th>53</th>
      <td>43.0</td>
      <td>12.51</td>
      <td>7.42</td>
    </tr>
    <tr>
      <th>54</th>
      <td>43.5</td>
      <td>12.60</td>
      <td>8.14</td>
    </tr>
    <tr>
      <th>55</th>
      <td>44.0</td>
      <td>12.49</td>
      <td>7.60</td>
    </tr>
  </tbody>
</table>
</div>




```python
# DataFrame -> Numpy로 변환. (머신러닝 학습을 위함)
perch_full = df.to_numpy()
perch_full
```




    array([[ 8.4 ,  2.11,  1.41],
           [13.7 ,  3.53,  2.  ],
           [15.  ,  3.82,  2.43],
           [16.2 ,  4.59,  2.63],
           [17.4 ,  4.59,  2.94],
           [18.  ,  5.22,  3.32],
           [18.7 ,  5.2 ,  3.12],
           [19.  ,  5.64,  3.05],
           [19.6 ,  5.14,  3.04],
           [20.  ,  5.08,  2.77],
           [21.  ,  5.69,  3.56],
           [21.  ,  5.92,  3.31],
           [21.  ,  5.69,  3.67],
           [21.3 ,  6.38,  3.53],
           [22.  ,  6.11,  3.41],
           [22.  ,  5.64,  3.52],
           [22.  ,  6.11,  3.52],
           [22.  ,  5.88,  3.52],
           [22.  ,  5.52,  4.  ],
           [22.5 ,  5.86,  3.62],
           [22.5 ,  6.79,  3.62],
           [22.7 ,  5.95,  3.63],
           [23.  ,  5.22,  3.63],
           [23.5 ,  6.28,  3.72],
           [24.  ,  7.29,  3.72],
           [24.  ,  6.38,  3.82],
           [24.6 ,  6.73,  4.17],
           [25.  ,  6.44,  3.68],
           [25.6 ,  6.56,  4.24],
           [26.5 ,  7.17,  4.14],
           [27.3 ,  8.32,  5.14],
           [27.5 ,  7.17,  4.34],
           [27.5 ,  7.05,  4.34],
           [27.5 ,  7.28,  4.57],
           [28.  ,  7.82,  4.2 ],
           [28.7 ,  7.59,  4.64],
           [30.  ,  7.62,  4.77],
           [32.8 , 10.03,  6.02],
           [34.5 , 10.26,  6.39],
           [35.  , 11.49,  7.8 ],
           [36.5 , 10.88,  6.86],
           [36.  , 10.61,  6.74],
           [37.  , 10.84,  6.26],
           [37.  , 10.57,  6.37],
           [39.  , 11.14,  7.49],
           [39.  , 11.14,  6.  ],
           [39.  , 12.43,  7.35],
           [40.  , 11.93,  7.11],
           [40.  , 11.73,  7.22],
           [40.  , 12.38,  7.46],
           [40.  , 11.14,  6.63],
           [42.  , 12.8 ,  6.87],
           [43.  , 11.93,  7.28],
           [43.  , 12.51,  7.42],
           [43.5 , 12.6 ,  8.14],
           [44.  , 12.49,  7.6 ]])




```python
# 타깃 데이터
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```


```python
#훈련데이터 테스트데이터 나누기.
train_data,test_data,train_target,test_target = train_test_split(perch_full,perch_weight,random_state = 42)
```

## 특성 생성하기


```python
#예시
poly = PolynomialFeatures()

poly.fit([[2,3]])
poly.transform([[2,3]])


```




    array([[1., 2., 3., 4., 6., 9.]])



4와 9 는 각 특성의 제곱

6은 두 특성의 곱

1은 (길이,높이,두께,1) 과 같이 특성과 곱해지는 계수이다!


```python
#농어의 특성을 만들어보자

poly = PolynomialFeatures(include_bias = False)
poly.fit(train_data)
train_poly = poly.transform(train_data)
print(train_data.shape)
print(train_poly.shape)
```

    (42, 3)
    (42, 9)


3개였던 특성이 9개로 늘어남을 알 수 있다!!!

그렇다면 특성이 어떻게 생성 되었을가 ?!


```python
poly.get_feature_names()
```

    C:\Anaconda3\lib\site-packages\sklearn\utils\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
      warnings.warn(msg, category=FutureWarning)





    ['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2', 'x2^2']




```python
#테스트 데이터의 특성도 만들어준다.

test_poly=poly.transform(test_data)

```

## 선형 회귀 - 다중 회귀


```python
lr = LinearRegression()
lr.fit(train_poly,train_target)
print(lr.score(train_poly,train_target))
print(lr.score(test_poly,test_target))
```

    0.9903183436982126
    0.9714559911594143


과소 적합을 문제는 해결이 되었지만 과대적합이 발생하였다!

즉, 훈련데이터에만 너무 잘맞는 머신이 되었다.(모델의 복잡도가 높다)

만약 여기서 특성의 차수를 더 증가시켜주면 어떻게 될까 ?! (과대적합 상승)

아래에서 확인해보자!


```python
poly = PolynomialFeatures(degree = 5, include_bias = False)

poly.fit(train_data)
train_poly = poly.transform(train_data)
test_poly = poly.transform(test_data)

print(train_poly.shape)
```

    (42, 55)



```python
poly.get_feature_names()
```

    C:\Anaconda3\lib\site-packages\sklearn\utils\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.
      warnings.warn(msg, category=FutureWarning)





    ['x0',
     'x1',
     'x2',
     'x0^2',
     'x0 x1',
     'x0 x2',
     'x1^2',
     'x1 x2',
     'x2^2',
     'x0^3',
     'x0^2 x1',
     'x0^2 x2',
     'x0 x1^2',
     'x0 x1 x2',
     'x0 x2^2',
     'x1^3',
     'x1^2 x2',
     'x1 x2^2',
     'x2^3',
     'x0^4',
     'x0^3 x1',
     'x0^3 x2',
     'x0^2 x1^2',
     'x0^2 x1 x2',
     'x0^2 x2^2',
     'x0 x1^3',
     'x0 x1^2 x2',
     'x0 x1 x2^2',
     'x0 x2^3',
     'x1^4',
     'x1^3 x2',
     'x1^2 x2^2',
     'x1 x2^3',
     'x2^4',
     'x0^5',
     'x0^4 x1',
     'x0^4 x2',
     'x0^3 x1^2',
     'x0^3 x1 x2',
     'x0^3 x2^2',
     'x0^2 x1^3',
     'x0^2 x1^2 x2',
     'x0^2 x1 x2^2',
     'x0^2 x2^3',
     'x0 x1^4',
     'x0 x1^3 x2',
     'x0 x1^2 x2^2',
     'x0 x1 x2^3',
     'x0 x2^4',
     'x1^5',
     'x1^4 x2',
     'x1^3 x2^2',
     'x1^2 x2^3',
     'x1 x2^4',
     'x2^5']




```python
# 55개의 특성으로 만들어진 훈련 과연 그 정확도는 ?!
lr.fit(train_poly,train_target)


print(lr.score(train_poly,train_target))
print(lr.score(test_poly,test_target))
```

    0.9999999999987451
    -144.4066764541083


특성 개수가 늘어나면 선형 모델은 엄청나게 강력해져서 세세한 특징들을 다 구별해냄!

즉, 테스트 데이터를 학습시키면 전혀 맞추지 못하는 현상이 일어남 (과대적합)

너무 정확도가 높아서 오히려 일반화되어 진 데이터를 구별해내지 못한다!

학습이 너무 잘 되어도 문제이다!

## 다중 회귀의 높은 과대적합을 규제 ?!
* 릿지(Ridge)
* 라쏘(Lasso)

각 특성들의 계수들의 스케일 차이가 너무크다면 ?!

스케일이 큰 특성들의 영향력이 커질 것이다. (k - 최근접 이웃알고리즘의 특성의 정규화와 비슷한 논리)


```python
#사이킷런의 변환기를 이용하여 특성들을 정규화

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

### 릿지(Ridge) 회귀
> 계수의 제곱한 값을 기준으로 규제를 적용


```python
from sklearn.linear_model import Ridge
ridge = Ridge()

ridge.fit(train_scaled,train_target)

print(ridge.score(train_scaled,train_target))
print(ridge.score(test_scaled,test_target))
```

    0.9896101671037343
    0.9790693977615391


Ridge 모델을 이용하여 규제를 하였더니 너무 과대적합이 발생하지 않게 되었다.

**릿지(Ridge),라쏘(Lasso)에서 규제 강도 또한 조절이 가능하다!**

+ alpha 값이 크면 계수의 값을 줄이게됨. 규제의 강도 증가!

+ alpha 값이 작으면 계수의 값을 줄이는 역할이 약해짐. 규제 강도 약화! (과대적합 가능성 증가)


```python
## alpha 값에 따른 R^2(결정계수)의 변화량을 알아보자 !

alpha_list = [0.001,0.01,0.1,1,10,100,100]
train_score = []
test_score = []

for alpha in alpha_list:
    ridge = Ridge(alpha = alpha)
    
    ridge.fit(train_scaled,train_target)
    train_score.append(ridge.score(train_scaled,train_target))
    test_score.append(ridge.score(test_scaled,test_target))
```


```python
plt.plot(np.log10(alpha_list),train_score)
plt.plot(np.log10(alpha_list),test_score)

plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```


![output_33_0](https://user-images.githubusercontent.com/97498405/158007585-05580417-d029-48d8-a001-e4568e489559.png)
    


-1 즉 10^ -1인

alpha = 0.1에서 가장 높은 성능을 보인다!

### 라쏘(Lasso) 회귀
> 계수의 절댓값을 기준으로 규제를 적용

> 라쏘는 계수를 줄이다가 0으로 만들 수 도 있다!(좋은 특성을 찾을 때도 사용됌.)


```python
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled,train_target)

print(lasso.score(train_scaled,train_target))
print(lasso.score(test_scaled,test_target))
```

    0.989789897208096
    0.9800593698421886



```python
## alpha 값에 따른 R^2(결정계수)의 변화량을 알아보자 !

alpha_list = [0.001,0.01,0.1,1,10,100,100]
train_score = []
test_score = []

for alpha in alpha_list:
    lasso = Lasso(alpha = alpha, max_iter = 10000)
    
    lasso.fit(train_scaled,train_target)
    train_score.append(lasso.score(train_scaled,train_target))
    test_score.append(lasso.score(test_scaled,test_target))
```

    C:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+04, tolerance: 5.183e+02
      model = cd_fast.enet_coordinate_descent(
    C:\Anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e+04, tolerance: 5.183e+02
      model = cd_fast.enet_coordinate_descent(



```python
plt.plot(np.log10(alpha_list),train_score)
plt.plot(np.log10(alpha_list),test_score)

plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```


​    ![output_38_0](https://user-images.githubusercontent.com/97498405/158007592-c5a1698c-36b1-456a-8bff-f565acb25b73.png)

1 즉 10^1인

alpha = 10 에서 가장 높은 성능을 보인다!


```python
# 사용되어진 특성의 갯수는 3개뿐이다!

np.sum(lasso.coef_==0)
```




    52

