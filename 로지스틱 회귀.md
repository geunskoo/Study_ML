# 목표

* k 최근접 알고리즘 분류 예측 확률의 한계
* 로지스틱 회귀를 이용한 예측 확률 구하기
* 예측 확률을 구하기위한/ 시그모이드(sigmoid) 함수와 소프트맥스(softmax) 함수 사용해보기

## 머신러닝에 필요한 패키지 삽입


```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
```

## 데이터 준비하기


```python
fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 7종의 물고기들이 있다!
pd.unique(fish['Species'])
```




    array(['Bream', 'Roach', 'Whitefish', 'Parkki', 'Perch', 'Pike', 'Smelt'],
          dtype=object)




```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_input[:5]
```




    array([[242.    ,  25.4   ,  30.    ,  11.52  ,   4.02  ],
           [290.    ,  26.3   ,  31.2   ,  12.48  ,   4.3056],
           [340.    ,  26.5   ,  31.1   ,  12.3778,   4.6961],
           [363.    ,  29.    ,  33.5   ,  12.73  ,   4.4555],
           [430.    ,  29.    ,  34.    ,  12.444 ,   5.134 ]])




```python
fish_target = fish[['Species']].to_numpy()
fish_target[:5]
```




    array([['Bream'],
           ['Bream'],
           ['Bream'],
           ['Bream'],
           ['Bream']], dtype=object)



## K  최근접 이웃 알고리즘을 이용한 `분류 확률` 구하기


```python
train_input, test_input,train_target, test_target = train_test_split(fish_input,fish_target,random_state=42)

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
kn = KNeighborsClassifier(n_neighbors = 3)
kn.fit(train_scaled,train_target)

print(kn.score(train_scaled,train_target))
print(kn.score(test_scaled,test_target))
```

    0.8907563025210085
    0.85


    C:\Anaconda3\lib\site-packages\sklearn\neighbors\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().
      return self._fit(X, y)


5개의 데이터를 출력해 확률을 확인해보자!


```python
#분류 클래스 순서
print(kn.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']



```python
print(kn.predict(test_scaled[:5]))
print(test_target[:5])
```

    ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
    [['Perch']
     ['Smelt']
     ['Pike']
     ['Whitefish']
     ['Perch']]



```python
proba = kn.predict_proba(test_scaled[:5])
np.round(proba,decimals=4)
```




    array([[0.    , 0.    , 1.    , 0.    , 0.    , 0.    , 0.    ],
           [0.    , 0.    , 0.    , 0.    , 0.    , 1.    , 0.    ],
           [0.    , 0.    , 0.    , 1.    , 0.    , 0.    , 0.    ],
           [0.    , 0.    , 0.6667, 0.    , 0.3333, 0.    , 0.    ],
           [0.    , 0.    , 0.6667, 0.    , 0.3333, 0.    , 0.    ]])



어떤 물고기가 나올지에대한 확률인데 0,0.3333,0.6667,1의 확률밖에 없다 ???

다른 모델을 사용해보자!

## 로지스틱 회귀(Logistic regression)

* 분류모델
* 알고리즈은 선형 회귀와 동일하게 선형 방정식을 학습한다.
* 시그모이드 / 로지스틱 함수를 이용하여 확률을 0~1사이의 값으로 변환한다!


```python
#시그모이드 함수 그리기
z = np.arange(-5,5,0.1)
phi = 1 / (1 + np.exp(-z))
plt.plot(z,phi)
plt.xlabel('x')
plt.ylabel('phi')
plt.show()
```


​    ![output_17_0](https://user-images.githubusercontent.com/97498405/158593475-363e33a1-43f5-4dc2-99f9-b0d5b388fd93.png)
​    


* LogisticRegression 을 이용하여 Binary Classification을 해보자!

### 로지스틱 회귀(Logistic regression) - Binary Classification


```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
bream_smelt_indexes = bream_smelt_indexes.reshape(119,)

train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```


```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt,target_bream_smelt)
print(lr.predict(train_bream_smelt[:5]))
```

    ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']


    C:\Anaconda3\lib\site-packages\sklearn\utils\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
      y = column_or_1d(y, warn=True)



```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

    [[0.99759855 0.00240145]
     [0.02735183 0.97264817]
     [0.99486072 0.00513928]
     [0.98584202 0.01415798]
     [0.99767269 0.00232731]]



```python
print(lr.classes_)
```

    ['Bream' 'Smelt']


첫번째 열은 음성클래스(0) 이고 두 번째열은 양성클래스(0) 이다!


따라서 위의 확률은 2번째 행빼고 모두 Bream으로 예측하고 있다!


```python
from scipy.special import expit

decisons = lr.decision_function(train_bream_smelt[:5])
print("각 요소들의 환산 되어진 z값들")
print("-------------------------------------------------------------------")
print(decisons)

print("로지스틱 함수를 거쳐 확률로 환산")
print("-------------------------------------------------------------------")
print(expit(decisons))
```

    각 요소들의 환산 되어진 z값들
    -------------------------------------------------------------------
    [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
    로지스틱 함수를 거쳐 확률로 환산
    -------------------------------------------------------------------
    [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]


### 로지스틱 회귀(Logistic regression) - Multiclass Classification


```python
lr = LogisticRegression(C = 20, max_iter = 1000)
lr.fit(train_scaled,train_target)

print(lr.score(train_scaled,train_target))
print(lr.score(test_scaled,test_target))
```

    0.9327731092436975
    0.925


    C:\Anaconda3\lib\site-packages\sklearn\utils\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
      y = column_or_1d(y, warn=True)



```python
print(lr.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']



```python
print(lr.predict(test_scaled[:5]))
```

    ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']


위 다섯개의 샘플에 대한 예측 확률을 출력해보자!


```python
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba,decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]


다중 분류의 선형 방정식은 어떤 형태로 생겼을까 ?!


```python
print(lr.coef_.shape,lr.intercept_.shape)
```

    (7, 5) (7,)


7종류의 물고기를 분류하고 5종류의 특성이 주어져있다!

그런데,다음 선형방정식의 계수의 갯수를 보면

클래스마다 z값을 계산한다는 것을 알 수 있다.

가장 높은 z값을 출력하는 클래스가 예측 클래스가 된다!

그렇다면 확률은 어떻게 구할까 ?

> 이진 분류 - 시그모이드 함수를 이용하여 z를 0과 1사이 값으로 변환

> 다중 분류 - 소프트맥스(softmax)함수를 이용하여 7개의 z값을 확률로 변환

소프트맥스(softmax) 함수를 이용한 z값 확률로 변환해보자!


```python
from scipy.special import softmax

decisons = lr.decision_function(test_scaled[:5])
print("각 요소들의 환산 되어진 z값들")
print("-------------------------------------------------------------------")
print(np.round(decisons,decimals = 2))

print("소프트맥스 함수를 거쳐 확률로 환산")
print("-------------------------------------------------------------------")
proba = softmax(decisons,axis = 1)
print(np.round(proba,decimals = 3))
```

    각 요소들의 환산 되어진 z값들
    -------------------------------------------------------------------
    [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
     [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
     [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
     [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
     [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
    소프트맥스 함수를 거쳐 확률로 환산
    -------------------------------------------------------------------
    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]

